{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.0\n",
      "  Downloading numpy-1.26.0-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.0-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/15.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/15.5 MB 3.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.8/15.5 MB 3.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.6/15.5 MB 3.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.1/15.5 MB 3.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.4/15.5 MB 2.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.9/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.5/15.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.7/15.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.0/15.5 MB 2.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.8/15.5 MB 2.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.8/15.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.9/15.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.7/15.5 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 9.4/15.5 MB 2.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.5/15.5 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 11.3/15.5 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 12.6/15.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\deep\\feature_extractor.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=torch.device(self.device))[\n"
     ]
    }
   ],
   "source": [
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.sort.tracker import Tracker\n",
    "\n",
    "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n",
    "tracker = DeepSort(model_path=deep_sort_weights, max_age=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Define the video path\n",
    "video_path = 'vid.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_path = 'output.mp4'\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "unique_track_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 233.0ms\n",
      "Speed: 15.0ms preprocess, 233.0ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 173.0ms\n",
      "Speed: 7.0ms preprocess, 173.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 558.8ms\n",
      "Speed: 5.0ms preprocess, 558.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 89.0ms\n",
      "Speed: 3.0ms preprocess, 89.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 161.0ms\n",
      "Speed: 3.0ms preprocess, 161.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 125.0ms\n",
      "Speed: 4.0ms preprocess, 125.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 118.0ms\n",
      "Speed: 3.0ms preprocess, 118.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 103.0ms\n",
      "Speed: 4.0ms preprocess, 103.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 123.0ms\n",
      "Speed: 3.3ms preprocess, 123.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 102.0ms\n",
      "Speed: 4.0ms preprocess, 102.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 111.0ms\n",
      "Speed: 3.0ms preprocess, 111.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 107.0ms\n",
      "Speed: 5.0ms preprocess, 107.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 88.0ms\n",
      "Speed: 3.0ms preprocess, 88.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 140.5ms\n",
      "Speed: 3.2ms preprocess, 140.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 112.0ms\n",
      "Speed: 5.0ms preprocess, 112.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 124.0ms\n",
      "Speed: 5.0ms preprocess, 124.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 126.0ms\n",
      "Speed: 5.0ms preprocess, 126.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 99.0ms\n",
      "Speed: 3.0ms preprocess, 99.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 195.9ms\n",
      "Speed: 5.0ms preprocess, 195.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 205.0ms\n",
      "Speed: 8.0ms preprocess, 205.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 141.0ms\n",
      "Speed: 5.0ms preprocess, 141.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 242.0ms\n",
      "Speed: 4.0ms preprocess, 242.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 308.7ms\n",
      "Speed: 8.0ms preprocess, 308.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 201.0ms\n",
      "Speed: 7.0ms preprocess, 201.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 161.0ms\n",
      "Speed: 5.0ms preprocess, 161.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 103.0ms\n",
      "Speed: 5.0ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 124.0ms\n",
      "Speed: 4.0ms preprocess, 124.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 148.4ms\n",
      "Speed: 5.0ms preprocess, 148.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 77.0ms\n",
      "Speed: 3.0ms preprocess, 77.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 148.0ms\n",
      "Speed: 4.0ms preprocess, 148.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 158.0ms\n",
      "Speed: 4.0ms preprocess, 158.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 356.0ms\n",
      "Speed: 12.0ms preprocess, 356.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 160.0ms\n",
      "Speed: 6.0ms preprocess, 160.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 173.0ms\n",
      "Speed: 5.0ms preprocess, 173.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 159.0ms\n",
      "Speed: 4.0ms preprocess, 159.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 197.0ms\n",
      "Speed: 6.0ms preprocess, 197.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 171.5ms\n",
      "Speed: 4.8ms preprocess, 171.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 105.0ms\n",
      "Speed: 4.0ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 106.0ms\n",
      "Speed: 4.0ms preprocess, 106.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 139.0ms\n",
      "Speed: 6.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 159.0ms\n",
      "Speed: 4.0ms preprocess, 159.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 153.0ms\n",
      "Speed: 5.8ms preprocess, 153.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 164.0ms\n",
      "Speed: 4.0ms preprocess, 164.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 198.0ms\n",
      "Speed: 5.0ms preprocess, 198.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 139.0ms\n",
      "Speed: 7.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 117.3ms\n",
      "Speed: 3.7ms preprocess, 117.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 189.0ms\n",
      "Speed: 6.0ms preprocess, 189.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 127.0ms\n",
      "Speed: 5.0ms preprocess, 127.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 108.0ms\n",
      "Speed: 4.0ms preprocess, 108.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 231.7ms\n",
      "Speed: 7.0ms preprocess, 231.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 70.0ms\n",
      "Speed: 3.0ms preprocess, 70.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 294.0ms\n",
      "Speed: 5.0ms preprocess, 294.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 184.0ms\n",
      "Speed: 6.0ms preprocess, 184.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 217.3ms\n",
      "Speed: 8.0ms preprocess, 217.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 115.0ms\n",
      "Speed: 5.0ms preprocess, 115.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 151.0ms\n",
      "Speed: 5.0ms preprocess, 151.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 184.0ms\n",
      "Speed: 6.0ms preprocess, 184.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 118.0ms\n",
      "Speed: 4.0ms preprocess, 118.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 201.8ms\n",
      "Speed: 7.0ms preprocess, 201.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 244.0ms\n",
      "Speed: 4.0ms preprocess, 244.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 371.0ms\n",
      "Speed: 44.0ms preprocess, 371.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 352.5ms\n",
      "Speed: 6.0ms preprocess, 352.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 205.0ms\n",
      "Speed: 7.0ms preprocess, 205.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 128.0ms\n",
      "Speed: 5.0ms preprocess, 128.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 185.0ms\n",
      "Speed: 4.0ms preprocess, 185.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 140.0ms\n",
      "Speed: 4.0ms preprocess, 140.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 169.9ms\n",
      "Speed: 24.0ms preprocess, 169.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 125.0ms\n",
      "Speed: 3.0ms preprocess, 125.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 137.0ms\n",
      "Speed: 8.0ms preprocess, 137.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 95.0ms\n",
      "Speed: 4.0ms preprocess, 95.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 141.0ms\n",
      "Speed: 3.0ms preprocess, 141.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 96.1ms\n",
      "Speed: 3.5ms preprocess, 96.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 123.0ms\n",
      "Speed: 4.0ms preprocess, 123.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 285.0ms\n",
      "Speed: 4.0ms preprocess, 285.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 156.0ms\n",
      "Speed: 5.0ms preprocess, 156.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 146.0ms\n",
      "Speed: 5.0ms preprocess, 146.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 208.2ms\n",
      "Speed: 4.5ms preprocess, 208.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 200.0ms\n",
      "Speed: 7.0ms preprocess, 200.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 215.0ms\n",
      "Speed: 6.0ms preprocess, 215.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 146.0ms\n",
      "Speed: 5.0ms preprocess, 146.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 114.8ms\n",
      "Speed: 3.0ms preprocess, 114.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 88.0ms\n",
      "Speed: 3.0ms preprocess, 88.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 133.0ms\n",
      "Speed: 3.0ms preprocess, 133.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 245.0ms\n",
      "Speed: 4.0ms preprocess, 245.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 104.0ms\n",
      "Speed: 4.0ms preprocess, 104.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 478.9ms\n",
      "Speed: 3.0ms preprocess, 478.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 267.0ms\n",
      "Speed: 5.0ms preprocess, 267.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 117.0ms\n",
      "Speed: 4.0ms preprocess, 117.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 333.0ms\n",
      "Speed: 9.0ms preprocess, 333.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 672.9ms\n",
      "Speed: 8.0ms preprocess, 672.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 302.0ms\n",
      "Speed: 5.0ms preprocess, 302.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 206.0ms\n",
      "Speed: 5.0ms preprocess, 206.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 126.4ms\n",
      "Speed: 5.0ms preprocess, 126.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 127.0ms\n",
      "Speed: 6.0ms preprocess, 127.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 134.0ms\n",
      "Speed: 5.0ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 148.0ms\n",
      "Speed: 6.0ms preprocess, 148.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 174.0ms\n",
      "Speed: 5.0ms preprocess, 174.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 180.2ms\n",
      "Speed: 5.6ms preprocess, 180.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 269.0ms\n",
      "Speed: 3.0ms preprocess, 269.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 122.0ms\n",
      "Speed: 4.0ms preprocess, 122.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 82.0ms\n",
      "Speed: 2.0ms preprocess, 82.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 175.0ms\n",
      "Speed: 4.0ms preprocess, 175.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 168.5ms\n",
      "Speed: 3.7ms preprocess, 168.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 483.0ms\n",
      "Speed: 5.0ms preprocess, 483.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 176.0ms\n",
      "Speed: 5.0ms preprocess, 176.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 137.0ms\n",
      "Speed: 3.0ms preprocess, 137.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 216.1ms\n",
      "Speed: 6.1ms preprocess, 216.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 231.0ms\n",
      "Speed: 5.0ms preprocess, 231.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 268.0ms\n",
      "Speed: 8.0ms preprocess, 268.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 165.0ms\n",
      "Speed: 3.0ms preprocess, 165.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 102.0ms\n",
      "Speed: 5.0ms preprocess, 102.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 258.0ms\n",
      "Speed: 14.0ms preprocess, 258.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 244.0ms\n",
      "Speed: 6.0ms preprocess, 244.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 128.0ms\n",
      "Speed: 5.0ms preprocess, 128.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 106.3ms\n",
      "Speed: 2.7ms preprocess, 106.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 150.0ms\n",
      "Speed: 4.0ms preprocess, 150.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 116.0ms\n",
      "Speed: 4.0ms preprocess, 116.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 116.0ms\n",
      "Speed: 4.0ms preprocess, 116.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 98.0ms\n",
      "Speed: 4.0ms preprocess, 98.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 108.0ms\n",
      "Speed: 3.0ms preprocess, 108.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 120.0ms\n",
      "Speed: 4.0ms preprocess, 120.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 130.0ms\n",
      "Speed: 4.0ms preprocess, 130.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 133.0ms\n",
      "Speed: 4.0ms preprocess, 133.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 128.0ms\n",
      "Speed: 4.0ms preprocess, 128.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 138.0ms\n",
      "Speed: 4.0ms preprocess, 138.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 162.8ms\n",
      "Speed: 3.4ms preprocess, 162.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 149.0ms\n",
      "Speed: 5.0ms preprocess, 149.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 210.0ms\n",
      "Speed: 4.0ms preprocess, 210.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 97.0ms\n",
      "Speed: 3.0ms preprocess, 97.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 118.0ms\n",
      "Speed: 5.0ms preprocess, 118.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 137.0ms\n",
      "Speed: 4.0ms preprocess, 137.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 128.0ms\n",
      "Speed: 5.0ms preprocess, 128.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 88.0ms\n",
      "Speed: 3.0ms preprocess, 88.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 96.0ms\n",
      "Speed: 3.0ms preprocess, 96.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 86.0ms\n",
      "Speed: 4.0ms preprocess, 86.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 111.0ms\n",
      "Speed: 3.0ms preprocess, 111.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 132.0ms\n",
      "Speed: 6.0ms preprocess, 132.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 210.0ms\n",
      "Speed: 5.0ms preprocess, 210.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 144.0ms\n",
      "Speed: 5.0ms preprocess, 144.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 103.0ms\n",
      "Speed: 3.0ms preprocess, 103.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 231.6ms\n",
      "Speed: 3.0ms preprocess, 231.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 111.0ms\n",
      "Speed: 3.0ms preprocess, 111.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 187.0ms\n",
      "Speed: 5.0ms preprocess, 187.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 113.0ms\n",
      "Speed: 5.0ms preprocess, 113.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 152.0ms\n",
      "Speed: 4.0ms preprocess, 152.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 193.0ms\n",
      "Speed: 16.0ms preprocess, 193.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 158.0ms\n",
      "Speed: 5.0ms preprocess, 158.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 95.0ms\n",
      "Speed: 3.0ms preprocess, 95.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 90.0ms\n",
      "Speed: 3.0ms preprocess, 90.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 66.0ms\n",
      "Speed: 3.0ms preprocess, 66.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 647.4ms\n",
      "Speed: 8.0ms preprocess, 647.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 184.0ms\n",
      "Speed: 4.0ms preprocess, 184.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 336.0ms\n",
      "Speed: 5.0ms preprocess, 336.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 133.0ms\n",
      "Speed: 5.0ms preprocess, 133.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 172.5ms\n",
      "Speed: 8.3ms preprocess, 172.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 145.0ms\n",
      "Speed: 4.0ms preprocess, 145.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 133.0ms\n",
      "Speed: 5.0ms preprocess, 133.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 158.0ms\n",
      "Speed: 5.0ms preprocess, 158.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 199.5ms\n",
      "Speed: 6.0ms preprocess, 199.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 156.0ms\n",
      "Speed: 6.0ms preprocess, 156.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 165.0ms\n",
      "Speed: 4.0ms preprocess, 165.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 201.0ms\n",
      "Speed: 12.0ms preprocess, 201.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 103.0ms\n",
      "Speed: 3.0ms preprocess, 103.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 185.4ms\n",
      "Speed: 4.4ms preprocess, 185.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 126.0ms\n",
      "Speed: 7.0ms preprocess, 126.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 277.0ms\n",
      "Speed: 6.0ms preprocess, 277.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 116.0ms\n",
      "Speed: 4.0ms preprocess, 116.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 144.0ms\n",
      "Speed: 5.0ms preprocess, 144.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 241.5ms\n",
      "Speed: 3.6ms preprocess, 241.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 177.0ms\n",
      "Speed: 5.0ms preprocess, 177.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 306.0ms\n",
      "Speed: 4.0ms preprocess, 306.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 391.6ms\n",
      "Speed: 16.7ms preprocess, 391.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 154.0ms\n",
      "Speed: 5.0ms preprocess, 154.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 170.0ms\n",
      "Speed: 6.0ms preprocess, 170.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 156.0ms\n",
      "Speed: 5.0ms preprocess, 156.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 151.9ms\n",
      "Speed: 6.2ms preprocess, 151.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 178.0ms\n",
      "Speed: 7.0ms preprocess, 178.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 276.0ms\n",
      "Speed: 8.0ms preprocess, 276.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 240.0ms\n",
      "Speed: 5.0ms preprocess, 240.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 172.2ms\n",
      "Speed: 5.0ms preprocess, 172.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 158.0ms\n",
      "Speed: 3.0ms preprocess, 158.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 263.0ms\n",
      "Speed: 10.0ms preprocess, 263.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 145.0ms\n",
      "Speed: 3.0ms preprocess, 145.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 160.0ms\n",
      "Speed: 3.0ms preprocess, 160.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 180.0ms\n",
      "Speed: 6.0ms preprocess, 180.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 308.0ms\n",
      "Speed: 10.0ms preprocess, 308.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 160.0ms\n",
      "Speed: 6.0ms preprocess, 160.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 125.0ms\n",
      "Speed: 4.0ms preprocess, 125.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 187.5ms\n",
      "Speed: 4.5ms preprocess, 187.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 110.0ms\n",
      "Speed: 4.0ms preprocess, 110.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 312.0ms\n",
      "Speed: 6.0ms preprocess, 312.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 118.0ms\n",
      "Speed: 5.0ms preprocess, 118.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 132.8ms\n",
      "Speed: 3.7ms preprocess, 132.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.0ms\n",
      "Speed: 4.0ms preprocess, 100.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 235.0ms\n",
      "Speed: 6.0ms preprocess, 235.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 164.0ms\n",
      "Speed: 4.0ms preprocess, 164.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 134.0ms\n",
      "Speed: 4.0ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 126.6ms\n",
      "Speed: 3.0ms preprocess, 126.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 116.0ms\n",
      "Speed: 4.0ms preprocess, 116.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 195.0ms\n",
      "Speed: 8.0ms preprocess, 195.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 124.0ms\n",
      "Speed: 5.0ms preprocess, 124.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 153.0ms\n",
      "Speed: 5.0ms preprocess, 153.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 132.4ms\n",
      "Speed: 5.6ms preprocess, 132.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 118.0ms\n",
      "Speed: 6.0ms preprocess, 118.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 158.0ms\n",
      "Speed: 6.0ms preprocess, 158.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 133.0ms\n",
      "Speed: 5.0ms preprocess, 133.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 182.0ms\n",
      "Speed: 4.0ms preprocess, 182.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 146.0ms\n",
      "Speed: 7.0ms preprocess, 146.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 145.0ms\n",
      "Speed: 5.0ms preprocess, 145.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 189.0ms\n",
      "Speed: 6.0ms preprocess, 189.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 146.0ms\n",
      "Speed: 7.0ms preprocess, 146.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 138.0ms\n",
      "Speed: 6.0ms preprocess, 138.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 207.0ms\n",
      "Speed: 4.0ms preprocess, 207.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 170.0ms\n",
      "Speed: 4.0ms preprocess, 170.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 242.0ms\n",
      "Speed: 7.0ms preprocess, 242.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 149.1ms\n",
      "Speed: 3.6ms preprocess, 149.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 142.0ms\n",
      "Speed: 4.0ms preprocess, 142.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 158.0ms\n",
      "Speed: 6.0ms preprocess, 158.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 119.0ms\n",
      "Speed: 4.0ms preprocess, 119.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 112.0ms\n",
      "Speed: 3.0ms preprocess, 112.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 198.1ms\n",
      "Speed: 3.0ms preprocess, 198.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 300.0ms\n",
      "Speed: 5.0ms preprocess, 300.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 181.0ms\n",
      "Speed: 6.0ms preprocess, 181.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 206.0ms\n",
      "Speed: 5.0ms preprocess, 206.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 587.4ms\n",
      "Speed: 6.0ms preprocess, 587.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 302.0ms\n",
      "Speed: 5.0ms preprocess, 302.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 207.0ms\n",
      "Speed: 6.0ms preprocess, 207.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 208.0ms\n",
      "Speed: 5.0ms preprocess, 208.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 201.3ms\n",
      "Speed: 8.0ms preprocess, 201.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 131.0ms\n",
      "Speed: 5.0ms preprocess, 131.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 424.0ms\n",
      "Speed: 9.0ms preprocess, 424.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 601.3ms\n",
      "Speed: 5.2ms preprocess, 601.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m frame \u001b[38;5;241m=\u001b[39m og_frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/detect/train15/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:547\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[1;32mc:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\venv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:304\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\venv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:147\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[0;32m    145\u001b[0m model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[1;32m--> 147\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    149\u001b[0m     kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:207\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[0;32m    206\u001b[0m     m\u001b[38;5;241m.\u001b[39mfuse_convs()\n\u001b[1;32m--> 207\u001b[0m m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[0;32m    209\u001b[0m m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\venv\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:262\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[1;34m(conv, bn)\u001b[0m\n\u001b[0;32m    260\u001b[0m w_conv \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mview(conv\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    261\u001b[0m w_bn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(bn\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdiv(torch\u001b[38;5;241m.\u001b[39msqrt(bn\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m+\u001b[39m bn\u001b[38;5;241m.\u001b[39mrunning_var)))\n\u001b[1;32m--> 262\u001b[0m \u001b[43mfusedconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_bn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_conv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfusedconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Prepare spatial bias\u001b[39;00m\n\u001b[0;32m    265\u001b[0m b_conv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "i = 0\n",
    "counter, fps, elapsed = 0, 0, 0\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        \n",
    "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = og_frame.copy()\n",
    "\n",
    "        model = YOLO(\"runs/detect/train15/weights/best.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "        results = model.predict(source=frame, device='cpu', classes=0, conf=0.8)\n",
    "\n",
    "        class_names = ['person']\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes  # Boxes object for bbox outputs\n",
    "            probs = result.probs  # Class probabilities for classification outputs\n",
    "            cls = boxes.cls.tolist()  # Convert tensor to list\n",
    "            xyxy = boxes.xyxy\n",
    "            conf = boxes.conf\n",
    "            xywh = boxes.xywh  # box with xywh format, (N, 4)\n",
    "            for class_index in cls:\n",
    "                class_name = class_names[int(class_index)]\n",
    "                #print(\"Class:\", class_name)\n",
    "\n",
    "        pred_cls = np.array(cls)\n",
    "        conf = conf.detach().cpu().numpy()\n",
    "        xyxy = xyxy.detach().cpu().numpy()\n",
    "        bboxes_xywh = xywh\n",
    "        bboxes_xywh = xywh.cpu().numpy()\n",
    "        bboxes_xywh = np.array(bboxes_xywh, dtype=float)\n",
    "        \n",
    "        tracks = tracker.update(bboxes_xywh, conf, og_frame)\n",
    "        \n",
    "        for track in tracker.tracker.tracks:\n",
    "            track_id = track.track_id\n",
    "            hits = track.hits\n",
    "            x1, y1, x2, y2 = track.to_tlbr()  # Get bounding box coordinates in (x1, y1, x2, y2) format\n",
    "            w = x2 - x1  # Calculate width\n",
    "            h = y2 - y1  # Calculate height\n",
    "\n",
    "            # Set color values for red, blue, and green\n",
    "            red_color = (0, 0, 255)  # (B, G, R)\n",
    "            blue_color = (255, 0, 0)  # (B, G, R)\n",
    "            green_color = (0, 255, 0)  # (B, G, R)\n",
    "\n",
    "            # Determine color based on track_id\n",
    "            color_id = track_id % 3\n",
    "            if color_id == 0:\n",
    "                color = red_color\n",
    "            elif color_id == 1:\n",
    "                color = blue_color\n",
    "            else:\n",
    "                color = green_color\n",
    "\n",
    "            cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "\n",
    "            text_color = (0, 0, 0)  # Black color for text\n",
    "            cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)\n",
    "\n",
    "            # Add the track_id to the set of unique track IDs\n",
    "            unique_track_ids.add(track_id)\n",
    "\n",
    "        # Update the person count based on the number of unique track IDs\n",
    "        person_count = len(unique_track_ids)\n",
    "\n",
    "        # Update FPS and place on frame\n",
    "        current_time = time.perf_counter()\n",
    "        elapsed = (current_time - start_time)\n",
    "        counter += 1\n",
    "        if elapsed > 1:\n",
    "            fps = counter / elapsed\n",
    "            counter = 0\n",
    "            start_time = current_time\n",
    "\n",
    "        # Draw person count on frame\n",
    "        cv2.putText(og_frame, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Append the frame to the list\n",
    "        frames.append(og_frame)\n",
    "\n",
    "        # Write the frame to the output video file\n",
    "        out.write(cv2.cvtColor(og_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Show the frame\n",
    "        #cv2.imshow(\"Video\", og_frame)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path exists: test/MOT16-01/img1/\n",
      "Files in the path: ['000001.jpg', '000002.jpg', '000003.jpg', '000004.jpg', '000005.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_path = 'test/MOT16-01/img1/'\n",
    "\n",
    "# Check if the path exists and contains files\n",
    "if os.path.isdir(video_path):\n",
    "    print(f\"Path exists: {video_path}\")\n",
    "    files = os.listdir(video_path)\n",
    "    if files:\n",
    "        print(f\"Files in the path: {files[:5]}\")  # Print first 5 files for verification\n",
    "    else:\n",
    "        print(\"The folder is empty.\")\n",
    "else:\n",
    "    print(\"Path does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort.deep_sort.tracker import Tracker\n",
    "from deep_sort.deep_sort.nn_matching import NearestNeighborDistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image path\n",
    "image_path = 'test/MOT16-01/img1/'\n",
    "\n",
    "# Get list of image files\n",
    "image_files = sorted(os.listdir(image_path))\n",
    "\n",
    "# Initialize device (GPU/CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize YOLO model\n",
    "model = YOLO(\"runs/detect/train15/weights/best.pt\")  # Load pretrained YOLO model\n",
    "\n",
    "# Initialize DeepSORT with TensorFlow model\n",
    "deep_sort_weights = 'deep_sort/resources/networks/mars-small128.pb'  # Path to TensorFlow model\n",
    "\n",
    "# Initialize the metric with cosine distance\n",
    "metric = NearestNeighborDistanceMetric(\"cosine\", 0.7, None)\n",
    "\n",
    "# Create the tracker with the metric\n",
    "tracker = Tracker(metric=metric, max_age=70, n_init=3)\n",
    "\n",
    "# Read the first image to get the width and height\n",
    "frame = cv2.imread(os.path.join(image_path, image_files[0]))\n",
    "frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "# Set FPS manually (since it's images, not a video stream)\n",
    "fps = 30  # You can adjust this as needed\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_path = 'output4.avi'\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 211.2ms\n",
      "Speed: 13.2ms preprocess, 211.2ms inference, 22.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Detected 1 objects\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Results' object has no attribute 'features'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    This class encapsulates the functionality for handling detection, segmentation, pose estimation,\n    and classification results from YOLO models.\n\n    Attributes:\n        orig_img (numpy.ndarray): Original image as a numpy array.\n        orig_shape (Tuple[int, int]): Original image shape in (height, width) format.\n        boxes (Boxes | None): Object containing detection bounding boxes.\n        masks (Masks | None): Object containing detection masks.\n        probs (Probs | None): Object containing class probabilities for classification tasks.\n        keypoints (Keypoints | None): Object containing detected keypoints for each object.\n        obb (OBB | None): Object containing oriented bounding boxes.\n        speed (Dict[str, float | None]): Dictionary of preprocess, inference, and postprocess speeds.\n        names (Dict[int, str]): Dictionary mapping class IDs to class names.\n        path (str): Path to the image file.\n        _keys (Tuple[str, ...]): Tuple of attribute names for internal use.\n\n    Methods:\n        update: Updates object attributes with new detection results.\n        cpu: Returns a copy of the Results object with all tensors on CPU memory.\n        numpy: Returns a copy of the Results object with all tensors as numpy arrays.\n        cuda: Returns a copy of the Results object with all tensors on GPU memory.\n        to: Returns a copy of the Results object with tensors on a specified device and dtype.\n        new: Returns a new Results object with the same image, path, and names.\n        plot: Plots detection results on an input image, returning an annotated image.\n        show: Shows annotated results on screen.\n        save: Saves annotated results to file.\n        verbose: Returns a log string for each task, detailing detections and classifications.\n        save_txt: Saves detection results to a text file.\n        save_crop: Saves cropped detection images.\n        tojson: Converts detection results to JSON format.\n\n    Examples:\n        >>> results = model(\"path/to/image.jpg\")\n        >>> for result in results:\n        ...     print(result.boxes)  # Print detection boxes\n        ...     result.show()  # Display the annotated image\n        ...     result.save(filename=\"result.jpg\")  # Save annotated image\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     xywh\u001b[38;5;241m.\u001b[39mappend(boxes\u001b[38;5;241m.\u001b[39mxywh\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Append all box coordinates\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     conf\u001b[38;5;241m.\u001b[39mextend(boxes\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mtolist())  \u001b[38;5;66;03m# Append confidence values\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Flatten the xywh list if needed (if it's a list of lists)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m xywh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(xywh, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\venv\\Lib\\site-packages\\ultralytics\\utils\\__init__.py:220\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 220\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Results' object has no attribute 'features'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    This class encapsulates the functionality for handling detection, segmentation, pose estimation,\n    and classification results from YOLO models.\n\n    Attributes:\n        orig_img (numpy.ndarray): Original image as a numpy array.\n        orig_shape (Tuple[int, int]): Original image shape in (height, width) format.\n        boxes (Boxes | None): Object containing detection bounding boxes.\n        masks (Masks | None): Object containing detection masks.\n        probs (Probs | None): Object containing class probabilities for classification tasks.\n        keypoints (Keypoints | None): Object containing detected keypoints for each object.\n        obb (OBB | None): Object containing oriented bounding boxes.\n        speed (Dict[str, float | None]): Dictionary of preprocess, inference, and postprocess speeds.\n        names (Dict[int, str]): Dictionary mapping class IDs to class names.\n        path (str): Path to the image file.\n        _keys (Tuple[str, ...]): Tuple of attribute names for internal use.\n\n    Methods:\n        update: Updates object attributes with new detection results.\n        cpu: Returns a copy of the Results object with all tensors on CPU memory.\n        numpy: Returns a copy of the Results object with all tensors as numpy arrays.\n        cuda: Returns a copy of the Results object with all tensors on GPU memory.\n        to: Returns a copy of the Results object with tensors on a specified device and dtype.\n        new: Returns a new Results object with the same image, path, and names.\n        plot: Plots detection results on an input image, returning an annotated image.\n        show: Shows annotated results on screen.\n        save: Saves annotated results to file.\n        verbose: Returns a log string for each task, detailing detections and classifications.\n        save_txt: Saves detection results to a text file.\n        save_crop: Saves cropped detection images.\n        tojson: Converts detection results to JSON format.\n\n    Examples:\n        >>> results = model(\"path/to/image.jpg\")\n        >>> for result in results:\n        ...     print(result.boxes)  # Print detection boxes\n        ...     result.show()  # Display the annotated image\n        ...     result.save(filename=\"result.jpg\")  # Save annotated image\n    "
     ]
    }
   ],
   "source": [
    "from deep_sort.deep_sort.detection import Detection\n",
    "\n",
    "for image_file in image_files:\n",
    "    img_path = os.path.join(image_path, image_file)\n",
    "    frame = cv2.imread(img_path)\n",
    "\n",
    "    if frame is not None:\n",
    "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB for YOLO\n",
    "        frame = og_frame.copy()\n",
    "\n",
    "        # Perform object detection\n",
    "        results = model.predict(source=frame, device=device, classes=0, conf=0.5)\n",
    "        \n",
    "        # Check if any boxes were detected\n",
    "        if len(results[0].boxes) == 0:\n",
    "            print(\"No objects detected in this frame\")\n",
    "        else:\n",
    "            print(f\"Detected {len(results[0].boxes)} objects\")\n",
    "\n",
    "        # Extract bounding boxes and confidence values\n",
    "        conf = []\n",
    "        xywh = []\n",
    "        features = []\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            xywh.append(boxes.xywh.cpu().numpy())  # Append all box coordinates\n",
    "            conf.extend(boxes.conf.tolist())  # Append confidence values\n",
    "            features.append(result.features.cpu().numpy())\n",
    "\n",
    "        # Flatten the xywh list if needed (if it's a list of lists)\n",
    "        xywh = np.concatenate(xywh, axis=0)\n",
    "        features = np.concatenate(features, axis=0)\n",
    "\n",
    "        if len(xywh) == 0:\n",
    "            continue  # Skip this frame if no detections\n",
    "        \n",
    "        # Create list of Detection objects\n",
    "        detections = []\n",
    "        for box, c in zip(xywh, conf):\n",
    "            detection = Detection(box, c)  # Create Detection object\n",
    "            detections.append(detection)\n",
    "\n",
    "        # Update tracker with bounding boxes and confidences\n",
    "        tracks = tracker.update(detections)\n",
    "\n",
    "        # Loop through tracked objects and draw bounding boxes\n",
    "        for track in tracker.tracker.tracks:\n",
    "            track_id = track.track_id\n",
    "            x1, y1, x2, y2 = track.to_tlbr()  # Convert to top-left/bottom-right format\n",
    "            color = (0, 255, 0)  # Green for bounding boxes\n",
    "            cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "            cv2.putText(og_frame, f\"ID-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(cv2.cvtColor(og_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Release the VideoWriter and cleanup\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video file\n",
    "video_path = 'output3.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    # Loop to read frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # If frame is read correctly\n",
    "        if ret:\n",
    "            cv2.imshow('Video Playback', frame)\n",
    "            \n",
    "            # Break on key press (e.g., pressing 'q' to quit)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Release the video capture object and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 00001\n",
      "\n",
      "0: 384x640 44 persons, 95.6ms\n",
      "Speed: 4.8ms preprocess, 95.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\deep_sort_app2.py\", line 125, in <module>\n",
      "    run(\n",
      "  File \"c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\deep_sort_app2.py\", line 102, in run\n",
      "    visualizer.run(frame_callback)\n",
      "  File \"c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\application_util\\visualization.py\", line 100, in run\n",
      "    self.viewer.run(lambda: self._update_fun(frame_callback))\n",
      "  File \"c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\application_util\\image_viewer.py\", line 305, in run\n",
      "    self._terminate = not self._user_fun()\n",
      "                          ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\application_util\\visualization.py\", line 100, in <lambda>\n",
      "    self.viewer.run(lambda: self._update_fun(frame_callback))\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\application_util\\visualization.py\", line 105, in _update_fun\n",
      "    frame_callback(self, self.frame_idx)\n",
      "  File \"c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\deep_sort_app2.py\", line 72, in frame_callback\n",
      "    detections = create_detections_from_yolo(frame, min_detection_height)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\FARAZ\\Desktop\\Projects\\Computer-Vision\\deep_sort\\deep_sort_app2.py\", line 49, in create_detections_from_yolo\n",
      "    for *xyxy, conf, cls in results.xywh[0].cpu().numpy():\n",
      "                            ^^^^^^^^^^^^\n",
      "AttributeError: 'list' object has no attribute 'xywh'\n"
     ]
    }
   ],
   "source": [
    "!python deep_sort/deep_sort_app2.py --sequence_dir=test/MOT16-03 --detection_file=deep_sort/resources/detections/MOT16_POI_test/MOT16-03.npy --min_confidence=0.3 --nn_budget=100 --display=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
